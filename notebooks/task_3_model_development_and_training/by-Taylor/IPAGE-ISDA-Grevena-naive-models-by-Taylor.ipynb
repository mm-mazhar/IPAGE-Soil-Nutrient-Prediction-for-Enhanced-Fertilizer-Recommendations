{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2902a2dd-b1b5-4233-b71a-af366f8abc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee759f94-f81a-4c98-a986-6e4b74523d13",
   "metadata": {},
   "source": [
    "Ideas for feature engineering\n",
    "\n",
    "1.Look into ratios between key nutrients, maybe focus on those most correlated with the target nutrients\n",
    "\n",
    "2.Make composite nutrient indicators, like adding all the macro nutrients as a total, or micro nutrient total\n",
    "\n",
    "3.Split soil (knit) in to the constituent amounts of clay, sand and silt\n",
    "\n",
    "4.Look for nutrients that are largely affected by amount of clay, silt and clay and create composite features\n",
    "\n",
    "5.Use PCA and Kmeans with one hot encoded categorical variables and numerical data, then see if dropping the categorical columns helps with the model performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a8a0d-1056-441a-87ba-8b40717eb9e9",
   "metadata": {},
   "source": [
    "Specific features to experiment with\n",
    "\n",
    "N:S ratio\n",
    "\n",
    "N+K\n",
    "\n",
    "Maybe just do total macronutrients as a feature N+P+K\n",
    "\n",
    "Also try weighted averages and geometric means of NPK\n",
    "\n",
    "Ratios between the macro nutrients N/P N/K K/P\n",
    "\n",
    "Kmeans clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32d4a8a9-16a8-4f63-afe7-80f6f6c60f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipage = pd.read_csv('./ipage_scaled.csv')\n",
    "isda = pd.read_csv('./isda_scaled.csv')\n",
    "isda = isda.dropna()\n",
    "grevena = pd.read_csv('./grevena_scaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2244d35d-5c7f-4c60-a62d-1ebf90ccbef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models on ipage dataset:\n",
      "Linear Regression on ipage Performance:\n",
      "MSE: [0.55115819 1.09809533 1.09273011]\n",
      "MAE: [0.55087117 0.74940987 0.77552514]\n",
      "R² Score: 0.20443674367107761\n",
      "------------------------------\n",
      "Permutation Importance for Linear Regression:\n",
      "      Feature  Permutation Importance\n",
      "0    Nitrogen                0.360336\n",
      "1  Phosphorus                0.048618\n",
      "2   Potassium                0.024098\n",
      "4          pH                0.016393\n",
      "3      Sulfur                0.012451\n",
      "------------------------------\n",
      "Random Forest on ipage Performance:\n",
      "MSE: [0.49212619 1.055702   1.05695512]\n",
      "MAE: [0.54362925 0.74820521 0.71404176]\n",
      "R² Score: 0.24424425186763754\n",
      "------------------------------\n",
      "Permutation Importance for Random Forest:\n",
      "      Feature  Permutation Importance\n",
      "0    Nitrogen                0.486510\n",
      "3      Sulfur                0.082087\n",
      "1  Phosphorus                0.060164\n",
      "2   Potassium                0.033433\n",
      "4          pH                0.022459\n",
      "------------------------------\n",
      "XGBoost on ipage Performance:\n",
      "MSE: [0.60779395 1.12794289 1.28296171]\n",
      "MAE: [0.58292256 0.77417025 0.82804067]\n",
      "R² Score: 0.12415026452538705\n",
      "------------------------------\n",
      "Permutation Importance for XGBoost:\n",
      "      Feature  Permutation Importance\n",
      "0    Nitrogen                0.542599\n",
      "1  Phosphorus                0.137657\n",
      "2   Potassium                0.095209\n",
      "4          pH                0.053180\n",
      "3      Sulfur                0.045642\n",
      "------------------------------\n",
      "Evaluating models on isda dataset:\n",
      "Linear Regression on isda Performance:\n",
      "MSE: [0.05659183 0.64346181 0.65616475]\n",
      "MAE: [0.15829342 0.50450555 0.62668112]\n",
      "R² Score: 0.4171877391672497\n",
      "------------------------------\n",
      "Permutation Importance for Linear Regression:\n",
      "      Feature  Permutation Importance\n",
      "0    Nitrogen                0.634450\n",
      "2   Potassium                0.120288\n",
      "4          pH                0.040986\n",
      "3      Sulfur                0.022141\n",
      "1  Phosphorus                0.013891\n",
      "------------------------------\n",
      "Random Forest on isda Performance:\n",
      "MSE: [0.06402044 0.67771402 0.64420303]\n",
      "MAE: [0.15749828 0.48815421 0.64279119]\n",
      "R² Score: 0.4044164490064777\n",
      "------------------------------\n",
      "Permutation Importance for Random Forest:\n",
      "      Feature  Permutation Importance\n",
      "0    Nitrogen                0.517657\n",
      "2   Potassium                0.171166\n",
      "4          pH                0.062990\n",
      "3      Sulfur                0.029705\n",
      "1  Phosphorus                0.026048\n",
      "------------------------------\n",
      "XGBoost on isda Performance:\n",
      "MSE: [0.07676855 0.81176235 0.75174752]\n",
      "MAE: [0.164997   0.53654492 0.69100747]\n",
      "R² Score: 0.2951176716658806\n",
      "------------------------------\n",
      "Permutation Importance for XGBoost:\n",
      "      Feature  Permutation Importance\n",
      "0    Nitrogen                0.654499\n",
      "2   Potassium                0.141347\n",
      "3      Sulfur                0.046467\n",
      "4          pH                0.043673\n",
      "1  Phosphorus                0.026420\n",
      "------------------------------\n",
      "Evaluating models on grevena dataset:\n",
      "Linear Regression on grevena Performance:\n",
      "MSE: [0.05659183 0.64346181 0.65616475]\n",
      "MAE: [0.15829342 0.50450555 0.62668112]\n",
      "R² Score: 0.4171877391672497\n",
      "------------------------------\n",
      "Permutation Importance for Linear Regression:\n",
      "      Feature  Permutation Importance\n",
      "0    Nitrogen                0.634450\n",
      "2   Potassium                0.120288\n",
      "4          pH                0.040986\n",
      "3      Sulfur                0.022141\n",
      "1  Phosphorus                0.013891\n",
      "------------------------------\n",
      "Random Forest on grevena Performance:\n",
      "MSE: [0.06402044 0.67771402 0.64420303]\n",
      "MAE: [0.15749828 0.48815421 0.64279119]\n",
      "R² Score: 0.4044164490064777\n",
      "------------------------------\n",
      "Permutation Importance for Random Forest:\n",
      "      Feature  Permutation Importance\n",
      "0    Nitrogen                0.517657\n",
      "2   Potassium                0.171166\n",
      "4          pH                0.062990\n",
      "3      Sulfur                0.029705\n",
      "1  Phosphorus                0.026048\n",
      "------------------------------\n",
      "XGBoost on grevena Performance:\n",
      "MSE: [0.07676855 0.81176235 0.75174752]\n",
      "MAE: [0.164997   0.53654492 0.69100747]\n",
      "R² Score: 0.2951176716658806\n",
      "------------------------------\n",
      "Permutation Importance for XGBoost:\n",
      "      Feature  Permutation Importance\n",
      "0    Nitrogen                0.654499\n",
      "2   Potassium                0.141347\n",
      "3      Sulfur                0.046467\n",
      "4          pH                0.043673\n",
      "1  Phosphorus                0.026420\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Split datasets\n",
    "ipage_X = ipage.drop(columns=['SOC', 'Boron', 'Zinc'])\n",
    "isda_X = isda.drop(columns=['SOC', 'Boron', 'Zinc'])\n",
    "grevena_X = isda.drop(columns=['SOC', 'Boron', 'Zinc'])\n",
    "ipage_y = ipage[['SOC', 'Boron', 'Zinc']]\n",
    "isda_y = isda[['SOC', 'Boron', 'Zinc']]\n",
    "grevena_y = isda[['SOC', 'Boron', 'Zinc']]\n",
    "\n",
    "# Train/test split\n",
    "ipage_X_train, ipage_X_test, ipage_y_train, ipage_y_test = train_test_split(ipage_X, ipage_y, test_size=0.2, random_state=0)\n",
    "isda_X_train, isda_X_test, isda_y_train, isda_y_test = train_test_split(isda_X, isda_y, test_size=0.2, random_state=0)\n",
    "grevena_X_train, grevena_X_test, grevena_y_train, grevena_y_test = train_test_split(grevena_X, grevena_y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Initialize dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Define function for evaluating models\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mse = mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "    mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "    r2 = r2_score(y_true, y_pred, multioutput='variance_weighted')\n",
    "    \n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"R² Score: {r2}\")\n",
    "    \n",
    "    results[model_name] = {'MSE': mse, 'MAE': mae, 'R² Score': r2}\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Define models to use\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(objective='reg:squarederror')\n",
    "}\n",
    "\n",
    "# Loop through datasets\n",
    "for dataset_name, (X_train, X_test, y_train, y_test) in {\n",
    "    \"ipage\": (ipage_X_train, ipage_X_test, ipage_y_train, ipage_y_test),\n",
    "    \"isda\": (isda_X_train, isda_X_test, isda_y_train, isda_y_test),\n",
    "    \"grevena\": (grevena_X_train, grevena_X_test, grevena_y_train, grevena_y_test)\n",
    "}.items():\n",
    "    \n",
    "    print(f\"Evaluating models on {dataset_name} dataset:\")\n",
    "    \n",
    "    # Loop through models\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        evaluate_model(y_test, y_pred, f\"{model_name} on {dataset_name}\")\n",
    "\n",
    "        perm_importance = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Permutation Importance': perm_importance.importances_mean\n",
    "        }).sort_values(by='Permutation Importance', ascending=False)\n",
    "        print(f\"Permutation Importance for {model_name}:\")\n",
    "        print(importance_df)\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a99519-14f9-43bc-84f0-0498e5e36bc0",
   "metadata": {},
   "source": [
    "It appears that SOC is easier to predict than Zn and B with just our numerical categories, the ipage dataset wasn't as easy to predict using just the numerical variables as the isda dataset. This is in line with the correlation matrices I saw during my EDA where the isda data showed much stronger correlations than the ipage. It reassuring to see that in both datasets Nitrogen was the most important feature, but I'd like to see whether some engineered features that capture more of the variance can be achieved.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- Nitrogen is the most important numerical variable\n",
    "- Since SOC is important for the ability of soil to hold nutrients, maybe if we can achieve a low enough error in predicting SOC, we can use it in a subsequent model to predict Zn and B?\n",
    "- I'd like to incorporate the categorical variables, but I'd like to get sand, silt and clay as numerical variables and combine them with some of the nurtient variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
